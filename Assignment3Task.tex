\documentclass{ctexart}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{bm}
\DeclareMathOperator*\E{\mathbb{E}}
\DeclareMathOperator\sign{sgn}
\DeclareMathOperator\err{err}
\DeclareMathOperator\conv{conv}
%\DeclareMathOperator*{\Pr}{Pr}
\begin{document}
\title{第三次作业}
\author{赵丰，2017310711}
\maketitle
\textbf{P1.1(a)}
\begin{equation}
\hat{\mathcal{R}}_S(cG+d)=\E_{\bm{\sigma}}\left[\sup_{g\in G}\frac{\bm{\sigma}\cdot (c\bm{g}_S+d)}{m}\right]
\end{equation}
因为$\forall g \in G$,
\begin{equation}
\E_{\bm{\sigma}}\left[\frac{d}{m}\sum_{i=1}^m \sigma_i\right]=0
\end{equation}
所以
\begin{equation}
\E_{\bm{\sigma}}\left[\sup_{g\in G}\frac{\bm{\sigma}\cdot (c\bm{g}_S+d)}{m}\right]=
|c|\E_{\bm{\sigma}}\left[\frac{1}{m}\sum_{i=1}^m \sign(c)\sigma_i g(z_i)\right]
\end{equation}
因为$\sign(c)\sigma_i$与$\sigma_i$具有相同的分布，所以
\begin{equation}
\E_{\bm{\sigma}}\left[\sup_{g\in G}\frac{\bm{\sigma}\cdot (c\bm{g}_S+d)}{m}\right]=
|c|\hat{\mathcal{R}}_S(G)
\end{equation}
从而推出：
\begin{equation}
\hat{\mathcal{R}}_S(cG+d)=|c|\hat{\mathcal{R}}_S(G)
\end{equation}

\textbf{P1.1(b)}
\begin{equation}
\hat{\mathcal{R}}_S(\conv(G))=\E_{\bm{\sigma}}\left[\sup_{g_i\in G}\frac{\bm{\sigma}\cdot (\sum_{i=1}^n \alpha_i\bm{g}_i)}{m}\right]
\end{equation}
其中$\bm{g}_i=(g_i(z_1),\dots,g_i(z_m))^T$

一方面：
\begin{align*}
\E_{\bm{\sigma}}\left[\sup_{g_i\in G}\frac{\bm{\sigma}\cdot (\sum_{i=1}^n \alpha_i\bm{g}_i)}{m}\right]\leq &
\sum_{i=1}^n \alpha_i \E_{\bm{\sigma}}\left[\sup_{g_i\in G}\frac{\bm{\sigma}\cdot \bm{g}_i}{m}\right]\\
=& \sum_{i=1}^n \alpha_i \hat{\mathcal{R}}_S(G)\\
=& \hat{\mathcal{R}}_S(G)
\end{align*}
另一方面，取$n=1,\alpha_1=1$有
\begin{equation}
\E_{\bm{\sigma}}\left[\sup_{g_i\in G}\frac{\bm{\sigma}\cdot (\sum_{i=1}^n \alpha_i\bm{g}_i)}{m}\right]\geq 
\E_{\bm{\sigma}}\left[\sup_{g_1\in G}\frac{\bm{\sigma}\cdot (\alpha_1\bm{g}_1)}{m}\right]
= \hat{\mathcal{R}}_S(G)
\end{equation}
所以推出
\begin{equation}
\hat{\mathcal{R}}_S(\conv(G))=\hat{\mathcal{R}}_S(G)
\end{equation}

\textbf{P1.1(c)}
可以证明$\sup_{a\in A,b\in B}(a+b)=\sup_{a\in A}a+\sup_{b\in B}b$,所以
\begin{align*}
\hat{\mathcal{R}}_S(G_1+G_2)=&\E_{\bm{\sigma}}\left[\sup_{\substack{g_1\in G_1\\g_2\in G_2}}\frac{\bm{\sigma}\cdot (\bm{g}_1+\bm{g}_2)}{m}\right]\\
=&\E_{\bm{\sigma}}\left[\sup_{g_1\in G_1}\frac{\bm{\sigma}\cdot \bm{g}_1}{m}\right]+\E_{\bm{\sigma}}\left[\sup_{g_2\in G_2}\frac{\bm{\sigma}\cdot \bm{g}_2}{m}\right]\\
=& \hat{\mathcal{R}}_S(G_1)+\hat{\mathcal{R}}_S(G_2)
\end{align*}

\textbf{P1.2}
\begin{align}\notag
\hat{\mathcal{R}}_S(G)=&\E_{\bm{\sigma}}\left[\sup_{h\in H}\frac{\sum_{i=1}^m \sigma_i 1_{h(x_i)\neq y_i}}{m}\right]\\
=&\E_{\bm{\sigma}}\left[\sup_{h\in H}\frac{\sum_{i=1}^m \sigma_i \frac{1-h(x_i)y_i}{2}}{m}\right]
\end{align}
因为$|y_i|=1$,由\textbf{P1.1(a)}的结论
\begin{align}\notag
\E_{\bm{\sigma}}\left[\sup_{h\in H}\frac{\sum_{i=1}^m \sigma_i \frac{1-h(x_i)y_i}{2}}{m}\right]=&
\frac{1}{2}\E_{\bm{\sigma}}\left[\sup_{h\in H}\frac{\sum_{i=1}^m \sigma_i h(x_i)}{m}\right]\\
=&\frac{1}{2}\hat{\mathcal{R}}_{S_\mathcal{X}}(H)
\end{align}
因此推出
\begin{equation}
\hat{\mathcal{R}}_S(G)=\frac{1}{2}\hat{\mathcal{R}}_{S_\mathcal{X}}(H)
\end{equation}

\textbf{P2.1}
对于直线上$m$个不同的点，考虑含$k(1\leq k \leq m)$个正类有$m+1-k$种分法，外加全为负类的一种分法，有
\begin{equation}
\Pi_H(m)=1+\sum_{k=1}^m (m+1-k)=1+\frac{m(m+1)}{2}
\end{equation}
根据 Sauer引理
\begin{equation}
\Pi_H(m)\leq \binom{m}{0}+\binom{m}{1}+\binom{m}{2}=1+\frac{m(m+1)}{2}
\end{equation}
因此对于数轴上区间作为假设集的例子，引理给出的上界是紧的。

 \textbf{P2.2}
 从$\{1,2,\dots,m\}$中选不超过$d$个数共有$K=\sum_{k=0}^d\binom{m}{k}$中取法，对应于第$i$种取法，$h_i(x)$是这样一个向量，将取出的数对应的分量位置置1，
 其余位置置0。注意$h_i$虽然是从$\mathcal{X}$到$\{0,1\}^m$空间的一个映射但却不依赖于$x\in \mathcal{X}$。将这$K$个映射构成我们
 考虑的函数空间$H$，由定义可知$H$的增长函数为$K$。当$m=d$时$K=2^d$，由VC维的定义可知$H$的VC维为$d$。所以我们构造出了这样一个
 假设集合使得Sauer 引理取到了等号。

 \textbf{P2.3}
 只需对$\mathbb{R}^n$中证明对任意$n+3$个点只用一个闭球均不能完全分离两类。
 反设$\exists \bm{x}_1,\dots,\bm{x}_{n+3}$，有
 $\forall y_i \in \{0,1\},1\leq i\leq n+3$,均$\exists r,\bm{x}^*$,
 使得$y_i(||\bm{x}_i-\bm{x}^*||_2^2-r^2)>0,1\leq i\leq n+3$

 下面
考虑$\mathbb{R}^{n+1}$中的点集,$\tilde{\bm{x}}_1,\dots,\tilde{\bm{x}}_{n+3}$,
其中$\tilde{\bm{x}}_i=\binom{\bm{x}_i}{||\bm{x}_i||_2^2}$,
对于这$n+3$个$\mathbb{R}^{n+1}$中的点，由前面假设：

$\forall y_i \in \{0,1\},1\leq i\leq n+3$,
构造$\bm{a}=\binom{-2\bm{x}^*}{1},b=||\bm{x}^*||_2^2-r^2$，则有
$y_i(\bm{a}\cdot \tilde{\bm{x}}_i+b)>0,1\leq i\leq n+3$，即用超平面在$n+1$维空间中对任意$n+3$个点都可以完全分离。
这与超平面的VC维等于考虑的空间维数加2矛盾。

因此用$\mathbb{R}^n$中的闭球分类VC维至多为$n+2$。

\textbf{P3(a)}
% \begin{align*}
% \E[\widehat{\err}(\hat{h})]=&\E_{S\sim D^m}[\frac{1}{m}\sum_{i=1}^m 1_{\hat{h}(x_i)\neq y_i} ]\\
% =&\sum_{\substack{(x_i,y_i)\in X\times(0,1)\\i=1,\dots,m}} (\frac{1}{m}\sum_{i=1}^m 1_{\hat{h}(x_i)\neq y_i})\Pi_{i=1}^m p(x_i,y_i)\\
% \end{align*}
对于给定的样本，由$\hat{h}$定义我们有
\begin{equation}
\E[\widehat{\err}(\hat{h})]\leq \E[\widehat{\err}(h^*)]=\err(h^*)
\end{equation}
其中最后一式利用了对于给定的假设，经验误差的均值等于泛化误差。
% \begin{equation}
% \frac{1}{m}\sum_{i=1}^m 1_{\hat{h}(x_i)\neq y_i}\leq \frac{1}{m}\sum_{i=1}^m 1_{h^*(x_i)\neq y_i}
% \end{equation}
% 所以
% \begin{align*}
% \E[\widehat{\err}(\hat{h})]\leq &\sum_{\substack{(x_i,y_i)\in X\times(0,1)\\i=1,\dots,m}} (\frac{1}{m}\sum_{i=1}^m 1_{h^*(x_i)\neq y_i})\Pi_{i=1}^m p(x_i,y_i)\\\\
% =&\frac{1}{m}\sum_{i=1}^m \sum_{(x_i,y_i)\in X\times(0,1)} 1_{h^*(x_i)\neq y_i} p(x_i,y_i)\\
% = &  \sum_{(x,y)\in X\times(0,1)} 1_{h^*(x)\neq y} p(x,y)\\
% = & \Pr_{(x,y)\sim D}(h^*(x)\neq y)\\
% = & \err(h^*)
% \end{align*}

另一方面，
% \begin{align*}
% \E[\err(\hat{h})]=&\E_{S\sim D^m} \left[\Pr_{(x,y)\sim D}[\hat{h}(x)\neq y]\right]\\
% \end{align*}
对于给定的样本，由$h^*$定义我们有
% \begin{equation}
% \Pr_{(x,y)\sim D}[\hat{h}(x)\neq y]\geq \Pr_{(x,y)\sim D}[h^*(x)\neq y]
% \end{equation}
% 而$h^*$与样本$S$无关，取期望仍为自身。即
\begin{equation}
\E[\err(\hat{h})]\geq \E[\err(h^*)]=\err(h^*)
\end{equation}

\textbf{P3(b)}
设$\tilde{S}$与$S$只有一个样本点不同，设其指标为$(x'_i,y'_i)$,记函数
\begin{equation}
\Phi(S)=\widehat{\err}(\hat{h})-\E[\widehat{\err}(\hat{h})]
\end{equation}
\begin{align}
\Phi(S)-\Phi(S')=& \min_{h\in \mathcal{H},S}\widehat{\err}(h)-\min_{h\in \mathcal{H},S'}\widehat{\err}(h)\\
\geq &\min_{h\in \mathcal{H}} \frac{1}{m}(1_{h(x_i)\neq y_i}-1_{h(x'_i)\neq y'_i})\\
\geq & \frac{-1}{m}
\end{align}
同理
\begin{equation}
\Phi(S')-\Phi(S)\geq \frac{-1}{m}
\end{equation}
从而有

\begin{equation}
|\Phi(S)-\Phi(S')|\leq \frac{1}{m}
\end{equation}
由McDiarmid不等式，有$1-\delta$的概率使得：
\begin{equation}
|\Phi(S)-\Phi(S')|\leq \sqrt{\frac{\ln(2/\delta)}{2m}} 
\end{equation}
由于$\delta$是小量,$\frac{\ln(2)}{m}$相比$\frac{\ln(1/\delta)}{m}$为小量，因此

\begin{equation}
|\Phi(S)-\Phi(S')|=O(\frac{\sqrt{\ln(1/\delta)}}{m})
\end{equation}
常数为$\frac{\sqrt{2}}{2}$
\begin{equation}
\end{equation}

\end{document}
